{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous:  [Process Monet](pipeline_prepare_monet.ipynb)  ::: Next [pipeline_tuning](pipeline_tuning.ipynb) \n",
    "# Example: processing the Movie stimulus\n",
    "In this notebook, we retrieve spikes inferred from the calcium traces and the synchronized movie stimulus, which may include static frames from the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import datajoint as dj\n",
    "from pipeline import preprocess, vis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider the more complex case wherein several different types of stimuli used during each calcium trace scan: `vis.MovieStillCond`, `vis.MovieClipCond`, `vis.MovieSeqCond`. \n",
    "\n",
    "All these conditions rely on movie information cached in lookup tables `vis.Movie`, `vis.MovieClip`, and `vis.MovieStill`.\n",
    "\n",
    "Here are the relevant tables in the `vis` schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dj.ERD(vis.Movie)+2+vis.Condition+vis.Trial-1).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following query denotes all trials combined with synchronization information to their two-photon scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = preprocess.Sync() * vis.Trial() & 'trial_idx between first_trial and last_trial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step of retrieving the spike traces was explained in detail in [Process Monet](pipeline_prepare_monet.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_conditions = [vis.MovieSeqCond(), vis.MovieClipCond(), vis.MovieStillCond()]\n",
    "datasets = (preprocess.Spikes() & \n",
    "            (trials & movie_conditions) & \n",
    "            dict(extract_method=2, spike_method=5))\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's fetch the primary key values of these datasets so that we can address them individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(datasets.fetch.keys())     # keys for relevant datasets\n",
    "key = keys[1]   #  identifies one dataset\n",
    "dict(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now let's fetch all the relevant information for the dataset identified by `key`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = (preprocess.Sync() & key).fetch1['frame_times'].squeeze()  # calcium scan frame times\n",
    "traces = np.vstack((preprocess.Spikes.RateTrace() & key).fetch['rate_trace'])  # fetch traces\n",
    "nslices = len(time)/traces.shape[1]   # number of slices in recording.\n",
    "time = time[::nslices]   # keep only one timestamp per frame (ignore time offsets of slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Now let's iterate through trials of each stimulus type separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie clip stimulus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate through the trials of type `vis.MovieClipCond`. \n",
    "Keep in mind that the trials of each type are interleaved.\n",
    "\n",
    "We are not processing each one now but will only show how to process the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clip_key in (trials * vis.MovieClipCond() & key).fetch.order_by('trial_idx').keys():\n",
    "    print('.', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's just look at the last trial addressed by `clip_key` (the last iteration of the loop above) and retrieve the stimulus movie shown during the trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_info = (vis.Trial()*vis.MovieClipCond()*vis.Movie.Clip() & clip_key).fetch1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_info['clip_number']   # the clip number. Some clips are repeated multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_info['cut_after']  # the cut time.  Some clips are cut short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `flip_times` field contains the time stamps of the frames of the movie on the same clock and same units as the `time` retrieved earlier for calcium traces from `preprocess.Sync`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_times = clip_info['flip_times'].flatten()   # the frame times of the visual stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the movie\n",
    "import io, imageio\n",
    "vid = imageio.get_reader(io.BytesIO(clip_info['clip'].tobytes()), 'ffmpeg')\n",
    "\n",
    "# show a frame from the movie\n",
    "plt.imshow(vid.get_data(50))\n",
    "grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may now relate the information in `traces` on `time` with the movie `vid` on `clip_info['flip_times']`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still frame stimulus\n",
    "Now let's process responses to stimuli comprising still frames from the movie.  Again let's iterate through all the trials but only look at the last one in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for still_key in (trials * vis.MovieStillCond() & key).fetch.order_by('trial_idx').keys():\n",
    "    print('.', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the last trial in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "still_info = (vis.Trial()*vis.MovieStillCond()*vis.Movie.Still() & still_key).fetch1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(still_info['still_frame'], cmap='gray')\n",
    "grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "still_info['flip_times'][0,0]  # frame time on the same clock as `time` for `traces`\n",
    "still_info['duration']   # duration on the screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence stimulus\n",
    "The sequence stimulus is still frame stimulus with fixed sequences of frames.  Again, we will iterate through all the trials and examine the last trial in depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_key in (trials * vis.MovieSeqCond() & key).fetch.order_by('trial_idx').keys():\n",
    "    print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_info = (vis.Trial()*vis.MovieSeqCond() & seq_key).fetch1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_info['seq_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_info['flip_times'].flatten()   # frame display times on the same clock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.uint32(seq_info['movie_still_ids'].flatten())   # Movie.Still id numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(4,int(ceil(seq_info['seq_length']/4)))\n",
    "for i, a in zip(seq_info['movie_still_ids'].flatten(), ax.flatten()):\n",
    "    im = (vis.MovieSeqCond()*vis.Movie.Still() & \n",
    "          dict(seq_key, still_id=i)).fetch1['still_frame']\n",
    "    a.imshow(im, cmap='gray')\n",
    "    a.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's it.  That is all it takes to retrieve the data for computing neuronal responses in response to the movie stimulus.\n",
    "\n",
    "Next [pipeline_tuning](pipeline_tuning.ipynb) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
